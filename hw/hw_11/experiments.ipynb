{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import mobilenet_v2, resnet18\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = 0\n",
    "\n",
    "def get_cls_model(input_shape):\n",
    "    \"\"\"\n",
    "    :param input_shape: tuple (n_rows, n_cols, n_channels)\n",
    "            input shape of image for classification\n",
    "    :return: nn model for classification\n",
    "    \"\"\"\n",
    "    device = torch.device(f'cuda:{CUDA}' if torch.cuda.is_available() else 'cpu')\n",
    "    model = mobilenet_v2(pretrained=False)\n",
    "    \n",
    "    downsample_coef = 224 * 224 // 1280\n",
    "    fc_size = input_shape[0] * input_shape[1] * input_shape[2] // downsample_coef\n",
    "    \n",
    "    model.features[0][0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "    model.classifier = nn.Sequential(\n",
    "        #nn.Linear(1280, 512),\n",
    "        #nn.BatchNorm1d(512),\n",
    "        #nn.ReLU(),\n",
    "        #nn.Linear(512, 256),\n",
    "        #nn.BatchNorm1d(256),\n",
    "        #nn.ReLU(),\n",
    "        nn.Linear(1280, 2)\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('tests/00_unittest_classifier_input/train_data.npz')\n",
    "X = data['X'].reshape(-1, 1, 40, 100)   #pytorch dimensions are (N, C, H, W)\n",
    "y = data['y'].astype(np.int64)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "X_train, X_test = torch.from_numpy(X_train), torch.from_numpy(X_test)\n",
    "y_train, y_test = torch.from_numpy(y_train), torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class SaveBestModel:\n",
    "    def __init__(self, best_val_loss=np.inf):\n",
    "        self.best_val_loss = best_val_loss\n",
    "        \n",
    "    def __call__(self, val_loss, epoch, model, optimizer, scheduler=None, model_path='model/best_model.pth'):\n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict() if scheduler else {}\n",
    "                }, model_path\n",
    "            )\n",
    "            print('New best model with loss {:.5f} is saved'.format(val_loss))\n",
    "\n",
    "def save_model(epoch, model, optimizer, model_path='model/final_model.pth'):\n",
    "    torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "                }, model_path\n",
    "    )\n",
    "    print('Model is saved')\n",
    "    \n",
    "def load_model(model, optimizer, scheduler=None, model_path='model/best_model.pth'):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    \n",
    "    if scheduler:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    return model, optimizer, epoch, scheduler\n",
    "\n",
    "            \n",
    "def train_epoch(model, optimizer, criterion, train_loader, device, tqdm_desc):\n",
    "    model.train()\n",
    "    train_acc, train_loss = 0.0, 0.0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=tqdm_desc):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc += (logits.argmax(dim=1) == labels).sum().item()\n",
    "        train_loss += loss.item() * labels.shape[0]\n",
    "\n",
    "    train_acc /= len(train_loader.dataset)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    return train_acc, train_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_epoch(model, criterion, val_loader, device, tqdm_desc):\n",
    "    model.eval()\n",
    "    val_acc, val_loss = 0.0, 0.0\n",
    "\n",
    "    for images, labels in tqdm(val_loader, desc=tqdm_desc):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        val_acc += (logits.argmax(dim=1) == labels).sum().item()\n",
    "        val_loss += loss.item() * labels.shape[0]\n",
    "\n",
    "    val_acc /= len(val_loader.dataset)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "\n",
    "    return val_acc, val_loss\n",
    "\n",
    "\n",
    "def train(model, optimizer, criterion, scheduler, train_loader, val_loader, device, num_epochs, model_saver, continue_training=True, model_path='model/best_model.pth', start_epoch=0):\n",
    "    \n",
    "    if continue_training:\n",
    "        model, optimizer, start_epoch, scheduler = load_model(model, optimizer, scheduler, model_path)\n",
    "    \n",
    "    for epoch in range(start_epoch + 1, num_epochs + 1):\n",
    "        train_acc, train_loss = train_epoch(model, optimizer, criterion, train_loader, device, f'Training epoch {epoch}/{num_epochs}')\n",
    "        val_acc, val_loss = val_epoch(model, criterion, val_loader, device, f'Validating epoch {epoch}/{num_epochs}')\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "        \n",
    "        print({'train_loss': train_loss, 'train_acc': train_acc, 'val_loss': val_loss, 'val_acc': val_acc})\n",
    "        model_saver(val_loss, epoch, model, optimizer, scheduler, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kirill-korolev\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kirill-korolev\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9885931558935361"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA = 0\n",
    "\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        device = torch.device(f'cuda:{CUDA}' if torch.cuda.is_available() else 'cpu')\n",
    "        model = resnet18(pretrained=False)\n",
    "\n",
    "        model.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        model.avgpool = nn.Identity()\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Linear(4096, 2),\n",
    "            #nn.BatchNorm1d(256),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Linear(256, 128),\n",
    "            #nn.BatchNorm1d(128),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Linear(128, 2)\n",
    "        )\n",
    "        \n",
    "        model = model.to(device)\n",
    "        self.model = model\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "        x = self.model.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.model.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def get_cls_model(input_shape):\n",
    "    \"\"\"\n",
    "    :param input_shape: tuple (n_rows, n_cols, n_channels)\n",
    "            input shape of image for classification\n",
    "    :return: nn model for classification\n",
    "    \"\"\"\n",
    "    device = torch.device(f'cuda:{CUDA}' if torch.cuda.is_available() else 'cpu')\n",
    "    return ClassificationModel()\n",
    "\n",
    "\n",
    "def train_epoch(model, optimizer, criterion, train_loader, device):\n",
    "    model.train()\n",
    "    train_acc, train_loss = 0.0, 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc += (logits.argmax(dim=1) == labels).sum().item()\n",
    "        train_loss += loss.item() * labels.shape[0]\n",
    "\n",
    "    train_acc /= len(train_loader.dataset)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    return train_acc, train_loss\n",
    "\n",
    "\n",
    "def train(model, optimizer, criterion, scheduler, train_loader, device, num_epochs):\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_acc, train_loss = train_epoch(model, optimizer, criterion, train_loader, device)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "def fit_cls_model(X, y):\n",
    "    \"\"\"\n",
    "    :param X: 4-dim tensor with training images\n",
    "    :param y: 1-dim tensor with labels for training\n",
    "    :return: trained nn model\n",
    "    \"\"\"\n",
    "    # your code here \\/\n",
    "    model = get_cls_model((40, 100, 1))\n",
    "\n",
    "    train_transform = T.Compose([\n",
    "        T.RandomHorizontalFlip(),\n",
    "        #T.RandomRotation((-5,5)),\n",
    "        T.Normalize(mean=0.5, std=0.5),\n",
    "    ])\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    train_dataset = TensorDataset(train_transform(X), y)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = None\n",
    "\n",
    "    n_epochs = 10\n",
    "    device = torch.device(f'cuda:{CUDA}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train(model, optimizer, criterion, scheduler, train_dataloader, device, n_epochs)\n",
    "\n",
    "    return model.cpu()\n",
    "\n",
    "model = fit_cls_model(X_train, y_train)\n",
    "y_predicted = torch.argmax(model.cpu()(X_test), dim=1)\n",
    "accuracy_score(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, 'classifier_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def linear_to_conv(layer):\n",
    "    out_channels = 2\n",
    "    in_channels = 512\n",
    "    kernel_size = (2, 4)\n",
    "    \n",
    "    weights = layer.state_dict()['weight'][:].view(out_channels, in_channels, kernel_size[0], kernel_size[1])\n",
    "    bias = layer.state_dict()['bias'][:]\n",
    "\n",
    "    conv = nn.Conv2d(in_channels=512, out_channels=2, kernel_size=kernel_size, bias=True)\n",
    "    conv.state_dict()['weight'][:] = weights\n",
    "    conv.state_dict()['bias'][:] = bias\n",
    "\n",
    "    return conv\n",
    "\n",
    "class DetectionModel(nn.Module):\n",
    "    def __init__(self, cls_model):\n",
    "        super().__init__()\n",
    "        self.cls_model = copy.deepcopy(cls_model)\n",
    "        self.cls_model.model.fc[0] = linear_to_conv(self.cls_model.model.fc[0])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        model = self.cls_model.model\n",
    "        \n",
    "        x = model.conv1(x)\n",
    "        x = model.bn1(x)\n",
    "        x = model.relu(x)\n",
    "        x = model.maxpool(x)\n",
    "        x = model.layer1(x)\n",
    "        x = model.layer2(x)\n",
    "        x = model.layer3(x)\n",
    "        x = model.layer4(x)\n",
    "        x = model.avgpool(x)\n",
    "        \n",
    "        x = model.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detection_model(cls_model):\n",
    "    \"\"\"\n",
    "    :param cls_model: trained cls model\n",
    "    :return: fully convolutional nn model with weights initialized from cls\n",
    "             model\n",
    "    \"\"\"\n",
    "    return DetectionModel(cls_model).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import read_for_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'tests/04_unittest_detector_input/'\n",
    "img_dir = os.path.join(test_dir, 'test_imgs')\n",
    "gt_path = os.path.join(test_dir, 'true_detections.json')\n",
    "images_detection = read_for_detection(img_dir, gt_path)\n",
    "\n",
    "images_detection_no_answer = {}\n",
    "images_detection_only_bboxes = {}\n",
    "for img_name, data in images_detection.items():\n",
    "    images_detection_no_answer[img_name] = data[0]\n",
    "    images_detection_only_bboxes[img_name] = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x43008 and 4096x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [493]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m220\u001b[39m,\u001b[38;5;241m370\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [439]\u001b[0m, in \u001b[0;36mClassificationModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     35\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[0;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x43008 and 4096x2)"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,1,220,370).to(device)\n",
    "model.to(device)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model = get_detection_model(model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imshow\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detections(detection_model, dictionary_of_images):\n",
    "    \"\"\"\n",
    "    :param detection_model: trained fully convolutional detector model\n",
    "    :param dictionary_of_images: dictionary of images in format\n",
    "        {filename: ndarray}\n",
    "    :return: detections in format {filename: detections}. detections is a N x 5\n",
    "        array, where N is number of detections. Each detection is described\n",
    "        using 5 numbers: [row, col, n_rows, n_cols, confidence].\n",
    "    \"\"\"\n",
    "    detections_dict = {}\n",
    "    threshold = 5\n",
    "    \n",
    "    nrows, ncols = 6, 9\n",
    "    box_nrows, box_ncols = 40, 100\n",
    "    \n",
    "    for image_filepath, orig_image in list(dictionary_of_images.items()):\n",
    "        image = np.zeros((220, 370))\n",
    "        image[:orig_image.shape[0], :orig_image.shape[1]] = orig_image\n",
    "\n",
    "        torch_image = torch.from_numpy(image)\n",
    "        torch_image = torch_image.view(1, 1, *image.shape).double()\n",
    "        feature_map = detection_model(torch_image.to(device)).squeeze(dim=0).detach().cpu().numpy()\n",
    "\n",
    "        pos_feature_map = feature_map[1,:]\n",
    "        \n",
    "        #imshow(image)\n",
    "        #plt.show()\n",
    "        #imshow(pos_feature_map)\n",
    "        boxes_idxs = np.argsort(pos_feature_map.ravel())[-threshold:]\n",
    "\n",
    "        row_indices = boxes_idxs // ncols\n",
    "        col_indices = boxes_idxs % ncols\n",
    "        \n",
    "        row = row_indices * box_nrows\n",
    "        col = col_indices * box_ncols\n",
    "        \n",
    "        detections = np.stack((row, col, np.full(threshold, box_nrows), np.full(threshold, box_ncols), pos_feature_map[row_indices, col_indices]), axis=1)\n",
    "\n",
    "        detections_dict[image_filepath] = detections\n",
    "\n",
    "    return detections_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([83, 142])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAggAAAE6CAYAAABzpqpYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/fElEQVR4nO2dXaxfxXnux9iG1mA74aNtwFZcjCEKggChkNiURFWLUFM1lVAqISGh6KiH60pppdxVvamaRIqUi0g5IVWRItE2JUdUxKGmIYEGQwlYfMjU2CWAbEwJAeoNmPgj3ucixyvPPMvr9cx4rf/e2/n9rubvWWtm1udefp/3Y9n8/Px8AgAAABDOWOgFAAAAwOKDDwQAAADowQcCAAAA9OADAQAAAHrwgQAAAAA9+EAAAACAHnwgAAAAQI8VrTseO3Ys7d+/P61evTotW7ZszDUBAADARMzPz6e33347XXjhhemMM4btBM0fCPv370/r169v3R0AAAAWkL1796Z169YN9jd/IKxevfoXE6SU1rQOAgAAzTz+wANN+/3ar/1a1/7Zz3422OfottF2pwN+fH6eom1b9pv6Opx11lld+5133kmbN2/u/o4P0fyBcFxWWJP4QAAAWAjOPvvspv1+/dd/vWsvX758sM/RbaPtTgf8D++KFcN/LqM/0qX7+XalY5Z+IJxou5O5BzR/IDjfue++wb6jR4/mkwYnbAx8fJ0/mrt1nWMdX+k6FxNnnnlm9vvw4cPF2/6qUvM/k9L9FvJ/czX/81ms+B+79957b/Qxo/FL/9hedfXVTftF1Iwx9UdBzb0z6/u/dY5T+QM+xPve977qdbz44otd+9133y3ahygGAAAA6MEHAgAAAPTgAwEAAAB6zEToXmg9fdY+Dws9zhCRv8BY/gH4GZyYSF+cWtucmlmso0bPH9qvxq9gaq19qTr4LaYIh8Vy/09F6bl+9dVXs7633nqraz/77LNd++mnn+7aR44cKVoDFgQAAADowQcCAAAA9FjwWLooJLFmvynWMjWtJn8PJdRto77TkVIzYxSK19pXM99SJwoDbDWXR6F+U8gBp6NZf4r9WllMJv9WKWQKmaR1TL1fn3vuuaxv69atXfuZZ57J+g4cOHDC8S688MLiuY+DBQEAAAB68IEAAAAAPfhAAAAAgB6LOsxxocMjS6nxJdC+KC1xzXylfaeybQtjpRSeYi1j9LXONwU1PgJj+A9MneK3dfzWe+5U5iidb9b39WJiap+f1rXU9E0xX3Sfq9/BnXfemfW98MILXXvt2rVZn/oabNy48YRjHzp0aHBeBQsCAAAA9OADAQAAAHqMZsNfKtkEnShEcIwxa/qWCkvF7D5rFlPWvinM92MzVojZrM36EUvxHp91ldBZm/jHYornW49Xqy2mlNLdd9/dtVVSSCmXFc4///ys77rrruva69evP+G8Bw8eDFb8S7AgAAAAQA8+EAAAAKAHHwgAAADQY2nEEZ6AKHzwZNsO7Xc6+Ae0spi1v8XE1OF9S4XFFLK3VO7d1nA+QidPTOQT0No3tF1NX0R0bp988sns98svv9y1PZRRufTSS7PfQ34HWtmxNPwXCwIAAAD04AMBAAAAeixZicFpDVdcTLJCZIIc2g7KGKvi4NTzTcGsw9hmzWIyz5dSM/cU62wN2RtjvlmPv9Ahv9H127FjR9d+8MEHi8fUio27d+/O+t55550TbvfTn/60ax85cqRoHiwIAAAA0IMPBAAAAOjBBwIAAAD0WHAfhNIQxJNt27LdqTC1frlUtOGoUuCpjDNEFLI0BQutUU7hLzDLe2uh/R+WynOktD5TrVU7F5O/zGJmjKqTP/zhD7Pf3/jGNwa3Pfvss7v2u+++m/Vp2KOnYdbfV155ZdfW61xa0gALAgAAAPTgAwEAAAB6jCYxTGHWX0xSQWnmqdORsUyeU6xlahbaRD1LKavVbNo631iMEbJXc1+Vmu7HYjGH8E1Naej3GOOfbI5Seeyss87Kfu/atatru6Swf//+rh1lS9RwxZRS2rhx4+C2Q2iY49GjR4v2wYIAAAAAPfhAAAAAgB58IAAAAECPBQ9znIKlop3OgkiHjLTUWVc5m4JZ6+uzZuzjW+hzUlqBr3SMqVhM9/hiojXldekYizks9pxzzunaqvWnlNK9997btZ977rms74Mf/GDXPv/887M+Hcf9E3yOIR5++OGurdfg2LFjRftjQQAAAIAefCAAAABAj0UtMdSYrBbaPDo2C20qXUxm1DGu7azNk7O+P6cef9b3w2K+/8YIt1voMU+H8NZZo+GLK1euzPr27NnTtf/xH/8x6/vud787OKZmS3Q8tFH57//+7649xb10HCwIAAAA0IMPBAAAAOjBBwIAAAD0mMwHYeoKdTX7taZiLd1vMemlY1Ga4vR08BOpqThYeu/O4ljHuK+XIq2a61gafemYrc9GzXMzxj24WJ/LsfDUx4cOHSra1v0MlPvvvz/7rSmUPZTxfe97X9fWsMaU8iqNmnY5pZT+53/+54RjpFR+3U/VHwELAgAAAPTgAwEAAAB6jCYx1JipFtKk1WpeXUxm2bFMrK3zTb3fQs8xa+lgqWQQHJuac9kqc7XO3yqDjmHWn8Vzo6Z0N7m7SV4pNc/XjBmNX7pfDdGY2jc3N5f1/eAHP+jaX/3qV7O+559/vmtH1+/ll1/Ofkf3mcoKnkkxCnMcQrf7+c9/XrQPFgQAAADowQcCAAAA9OADAQAAAHos6lTLzhha7UJTqheNFeq0kLSGgC1mpr7PFut93OovMNZ1nvX9MsZ8Nfq5au+tunsUzlequ59sv9K1jXUMpYzlu7Bt27aufc899wz2OdH9ov4CER7KqGGO2nZKfdCo5ggAAACjwAcCAAAA9BhNYohMo1NkIRzLFDtFxrRZZ4psHXPWJv8x5qi5z04HSSpijIxpp0Plvii8rnS/hWYKk3xpuKJvd84553TtI0eONK1r1uiaU8rX7RkRly9f3rXvvfferO9v/uZvurZLA1O8y1VWaJURovBIHbPlHYEFAQAAAHrwgQAAAAA9+EAAAACAHpOFOarmOwu9t7UaWst2J9t2DA12iipxJ5tjaqb2CYjGWGifg9L7s6ZvaLuFplX7nnpu+CWRn8GePXu69gc+8IGsb82aNV27xt9D/QBa/Rrcl0DH8TH1PvCQvn/913/t2nfdddfgfK3PlIcrKu5nEKVMbvURi6pAHufo0aNp7969Jx0LCwIAAAD04AMBAAAAekwmMYxh0l3o7G2tayllCjlgFqbmqeWAxcRiqha5VGSE1v3UZP3iiy9mfXrs73//+7O+t956q2tv2LAh6ys1bUfm6ynw+caYOxrzjDPy/wtqpUKtUphSSk899VTXvvbaa7O+G264oWur3JBSnJ1P56859mhb7fPj03tp69atWd8XvvCFru2hjL/1W7/VtSOpwFGzfmt4ZM3fg5a/d7ouMikCAABAM3wgAAAAQA8+EAAAAKDHZD4Ii7X62mLScVvX4vp9lMo62u90YDFdz6mJqu6NNWYpqv8+++yzWd8rr7zStVetWjU4xuHDh7Pfqqv+/Oc/H1zn0aNHsz7d9uWXX876VCdX3T2llC677LKurdpzSrH2PQatWruj58WPPQqLfe6557r2o48+mvVpCOQTTzyR9UVhqhdddFHXjvR79yHR+Vrvx2XLlmW/v/e973Vt9Tlw/LqXhAj6djVjOmP4J0TXubSS5BBYEAAAAKAHHwgAAADQYzSJYdbVAJdKVjmn1Mxfk2nwdJAOxsiEWUNrBcCW8X2Omr6aOcYmChFUWeGhhx7K+tRkfP7552d9GkLnJtxLLrmka69Ykb+a9BxpWGNKucn63Xffzfp27drVtfft25f16bZu9tbfvs4oI2hpxTzfrrTi7VXW99hjj3XtV199NevTTHlu5n7nnXe6tldC1L5169YNjunnU8epCRHU0FQPndRMjn6N9Jw9/fTTWd9Xv/rVru3HHq2ttLpiq/zg113lCN8vupdef/31wT6X7o5z3nnnDe4zBBYEAAAA6MEHAgAAAPTgAwEAAAB6LJufn59v2XFubi6tXbs2HUgprUkp7frP/xzcdqwqVVNQWmFwiuqDi4nFfI2mhgqAvyAKp3v88cez39u2bevarseuX7++ay9fvjzr8/BFxSsHKqqve1il6uQeYhb5LkQ6fIT6VUTH4/Ppfr/xG7+R9enz5uGYyqf/5E+y33/xuc917egYIs1cr1dK/bDHIaL59NymlNKBAwe69k9/+tOsb//+/V177dq1Wd/ZZ5/dtTdu3Dg45jPPPJP1aXif3xNTUFqVMSIKj4x8DlavXp39fvvtt0861/z8fHrrrbfSgQMHen4fChYEAAAA6MEHAgAAAPRYkDDHMUzWkYm/VQ44HUIJx5IDFlJWmEWI4GJlrEp3Y+y3Z8+eru0Sg1Jjno+ukT5jbp7X324uV3N2FLLqZm/FwzEj6UDlgTfffHNwuw9/+MPZ79b3y8GDBwf7asz8ip5DDV101Ix/sj7NpOh9muVRZYOUUrrwwgsH54hwqUKZhayglIZp+71bExKpuKxQ0lciPThYEAAAAKAHHwgAAADQgw8EAAAA6DFZNceIqcMHf9X8BaZgjFTEY/kEROMsRb8D9wFQH4FWfwFHx4nGOOOMX/4fwcPrtOKfa6UaWuhat+r5UQiV3/+qz/p7QdPs+nyRjhv1KZ7aWSs9egpjXcvFF1+c9f34xz8e7FPfheeffz7r0zk8Va5Xrxwi8jlwSs+Lhx2qb4H6HDie8rq0z/0TFA9l1LX4Ooe2G4sxKjSmVJ7aOfI5KEXHOHbsWM/P50RgQQAAAIAefCAAAABAj8kkhjFCC6dgMZv1IyKT/9RywGIy40fm+dYQwbHM+hE+R+lahrbzbb2v5Rh27NiR/daMel7VT83ZbhrVEEF/3tQ879X51AT/wQ9+MOtTk3xNdkYNA/T5oudGza+//du/nfWpHODZHzW08cUXX8z6VDbxEDaVFdwE78erXHHFFV3bwxX1ukTyQ02YamTK3717d9d+4YUXivdT/NhVHoiqMkYyQqv8ULpmX1tNJUudQ7MxOq3ZGU8VLAgAAADQgw8EAAAA6MEHAgAAAPQYzQeh1Zcg8gmIUlYuVcbwJVgq/gI11PgPDO1Xo9+XMpZ/Qul+NeOXbquhjCmltGzZsq790ksvde2HH3442061adep9+3bNzifVlt0XfxDH/pQ13afgGPHjnVt1911vw0bNmR9qt2Whu85r732WvZbQz6vvvrqwf1++MMfZr+14qD7J2gIpHPVVVd1bQ+rjMLRNJTSz5leW/VVSCm/Ru6fUJq++ZVXXsn6PDR1iEj3j/wMarT9UqLqkU4UnhmFK6o/jR97dC4Ww987LAgAAADQgw8EAAAA6DFZNccoDEO3dXOaZm9zc8/HPvaxprW1mu4XslLgUpEKWsMOnTFC9qYw/08R8liDywOlqIzgYYBamVGfvygUziWFUnOvm1t37do1uO1v/uZvdm2VFFLK3xn+rlHJwY9VK/65qV4rDDpnnnlm1/bws/Xr13dtz4io4X3nnntu1lcqw/qxR+8slTHc5B/JLXqt9+/fn/VpdUWXG3xbRTNoRub4iFYZoSaUUWUgrySpx+fVIqP7Jfp75/dIKTqmV6eMQiKH0OcmCg1WsCAAAABADz4QAAAAoAcfCAAAANBjNB8E1/dUM4nC+Twl5+233961b7311qwv0iiVVj+Dk207NjXjl2rhUahf65g1TB0GuFRRXwIN5/M+9R1IKdcK3UdAnzkPGVTt2MMXh8LmXG8urQ4YhcVF4ZGPPfZY1qehflGVV/dB0ON58803s77o+db9VD9PKdd/d+7cOTjGJz7xicH5ovBE16z1eKP9XGnX0Mknnngi61OfBL+Wqsv7uVYd/tJLL836/PcQ7oPg7/khatIbRyGC6r8Wbed96ncQ7ee+EpFPwBtvvDHYd9555w32lY7v6N9e3U/vOX8HDYEFAQAAAHrwgQAAAAA9RpMYvHKZZo9y87Gau37wgx9kfQcPHuza27Zty/puvvnmru0V1tasWTM4X2R2H0NGqJmvdD/vi8Ld1Fx0upvqnch0X7pfRCQH1OynRNUINcNdSnmGPw29Syl/Vo4ePZr1qRnezdl6z69Y8ctXgJtNo0qBGirpeOiYombbiy66KOt76qmnBteiUoWHSpaaryNcYrj++uu7tss3Tz/9dNf2Cph6bv2a6LmOQiA946JW1fyirfvb3/521/bzoGNGYYcefqmSgx5rSv3zpOgcLluodBCZ7p0os2E0jq7F70ddm4cylsoKLoXobw+HVBnB5bHWsE6VDjwEUnn77be79qZNm7r20aNHe8/0icCCAAAAAD34QAAAAIAefCAAAABAj9F8EO68887st4ZW3XTTTVmfpkz++Mc/nvWpPhul9YwqnrkmE1X8K+1zWv0MSrf1sCStGufhTDfeeOMJ2ym1a/RTE1UYjJifnx9lvjH2i/wT3M9AcZ8A1Zi1imBKuX+Ca9oaUhdVTbzsssuyvqFU55pCOKX8mfK0varjur4dVfVTDdv1V53fj+fBBx8cHD/S11XDjrbzPv3tOniktZemU/YxVcP2957r5Ir6Hfgx6DiumavWHvU5mgrfr1/ke6LUhDLqMdX4Luh9Ft2PPma0ttIxa0Ig9bmK0jVHlIZA6rH5u2QILAgAAADQgw8EAAAA6DGaxPAP//APg33f/e53s9+f//znu3aUNSzCQ3q+8pWvdO2/+qu/yvpKq0DWhBZOUQFQzdJ+Xr7xjW90bQ+tUpOWh3+q2bbVXB5Rs18kI6gp3c3LaobT6nUp5dk1W2WEGvQYamQEDc3zMCg9dh9T74OaaouXXHLJ4HxqttX53OytIZce7lZqgo/C1DxUUn9HMoKPGc1RWlXQx4hC4cbA3181ZnclOi9qPq8xzysuLem7xiUFlVciE3x0vVxeUfN5TYhgdM10/ui819xXen7d5K+hho4ek293+PDhwf0iPBz6OPoeoJojAAAANMMHAgAAAPTgAwEAAAB6jOaDsGrVquy3hmD96Ec/yvo+85nPdO3Vq1dnfZE28rWvfW2w7/vf/37X3rx5c9Z39dVXd+2a1Mo/+clPurZrapG2r7p8jS6ux+7pXbVq3AMPPJD1ffnLX+7aGzZsyPp0HE1H7bTq95EO7yGJmjbYz6dq3H4PRFULt2zZ0rXVH+Fka4ko9ZXwUNeXXnqpa2s4WEop3X///V378ssvz/r0XtIxUuprwEO4xqyhsO67oDqv6qy+Lq0GGGn5nn5XtWjXgkt9ApxIDy7dL9KNx6I1LK817DA6L6XpjWuqFioe0tl6jfR+9Ps9SiNc6rfRuq4o9NXPWfSc6t84DzlW3G8p8l2I0Pncb6MWLAgAAADQgw8EAAAA6DGaxOCV2dRc4qEraqaNTDNuvlazqYeA6JgeVqmZHFVuSCkPSfHsjG+++ebg2hStXJlSbq5/7LHHsj41kWu2u5TyMC+t0pZSfg6HwlhSyivipZRXkNu4cWPWpyGDLj9EZnY117scoCGKXuHTq9QpUQhftN19993XtT/84Q9nfRreWpqpMaW4uuK9997btXfv3p31qTndzZp6/SIzuz8PpefFza26X2RC1jXv3LlzcDuXEaIwNqU1BNIpzZZYQ2SinkJ+aCVaZ3Re9BiicMUaSSNaS2k4qK9Z1+aSRus1ivaLqlyWjhlVc2yVc1xOUTnCQ5X176vLFkMhlzo+mRQBAACgGT4QAAAAoAcfCAAAANBjNB8ED9FQvcNDLXRbT0upoWOub5eGfezZsyf7rSFnXi1P9XzXyDV97bXXXpv1qV4b6XueTlZD2vycbdu2rWv/3d/9XdZ38803d+1NmzZlfTq/hqallIcP3nHHHVmf+iD4eVF/DNe/Iv1Kw1tbic7nOeeck/Wp1q7nL6X8/H7yk5/M+jQUVSuPppSfi+985ztZn27remlUmU2fhxrNXI/X/RFUa3QfIN020obVt6BGi1Z8/KiiYWl1xRpKx4kq97Xq2a2+CjWplUvvl2gt7p+jRFVzowqNpX4oKeXXyN8nUfrmoTGc0oqGJxtHr4uv0/+uKB6ur+izHx2fh0ZHxxS9T0pSSeODAAAAAM3wgQAAAAA9RpMYXEaIwhc1LMNDO9Ss8sYbb2R9Bw8e7NpRBj+XIv7jP/5jcD7FzclqfnWJQU00bopSM79X2VP8GHS/Cy64IOu75ZZburZm5UsppWeeeaZrf+pTn8r69Fy7NKFmaA/H1HVHoXaRyd/RLI+l1cRSKs8m6HNv3bp1cFsNk3U5QMMj/b7Wa/38889nfddcc03XjsKn3OynxxedPzch6xwuLZWa3aPtWkMSS2kNnRxLmiiVFaKQ1RrGqNjojJElsDWMMiLKNOjPW+l8/rxF6N+RmhBPxdep74wo1DzKlhgdX5RJ0SWMSKqIJL7jeBbYIbAgAAAAQA8+EAAAAKAHHwgAAADQYzQfhBoi7U/1G99OQws9nE71fK9MeM8993Rt9xfQ364B6Vo0zXNKfZ8EJQohUR3eUzTfeuutXdvD1tTvwI9BdbSPfOQjWZ/O4Rqe/nafAE0J7bp4FHqnuPanlQrdd0H16MgvJfLp8DFvvPHGrq3+AU6UZvr3f//3sz69R2677basTytuajhrSnklUvUZcfye0GsbhSu6fq86pGv9Qzqo/3urvh35EkT6qPb5GKUphVupCa+L/BPG0OxrfBVa5/N7QtH7LAoNrSGqCtkaRqrvCR9D323R3DV+DV7tVIn8pIZSH58M9Ttwv7odO3Z07RafB8IcAQAAoBk+EAAAAKDHaBKDh1lpGIiHaKiZxcPIItTE41mtoiyL2vdv//ZvWZ9KE2eddVbWt3379q7tJjkNhXNp4p/+6Z+69iOPPJL1qRncTUOa7c9N4v/8z//ctT1joJravve972V9KgFE4T5uEn/rrbfSEJH8MDS+41KBjqlShHPFFVdkv7V65CWXXJL16Tl0+UHNqH6sWoXSs2s++OCDg2vTsEo3PZeakCNzdqlUcKL5h/rGChlUVCqIZJGScKzjROevNCOio/tNEUoYUbPOaNtobXpMfq5VwomOPZJexkKPwbMJRiGRUShj6d8VlzMjqSC6z3SdUYhnNF+UmdXRv2mRTKLz6fnSTLIRWBAAAACgBx8IAAAA0IMPBAAAAOgxmg+Chx1GqSiVSKd2VANyvwbVZCJ9xdMba0ikp59cuXJl13ZfAtXwXBt+8sknu7ani7777rsH13bTTTd17Ysvvjjr09+uY/3u7/7u4FpKUS3/VLZVXwbX9nXdHmazYsUvb0X3JdB7S30VUsrPi8/n/hhD+D2hPhBejfOFF14YHEdDYTUta0r58+AhsqrBenifpvWtqZ43NH5KbWmYXYvWZ9HPia65JixOjy9KZxxpvB4mWqrRt4bz1aRdjsZs1frHOIbofHoVyMi/RM91dN79nEWh5upLEPkj+N+RKBRV35H+LJT6ZkTnNvL1cvSYov28T9+lpano/f1b8s7HggAAAAA9+EAAAACAHqNJDB4iWIqHK3rFQUVNYW5mV4khCnn0LIsqR3ifSg4uFdx3331du+bY1Wy0bt26rE/N2T/5yU+yPs2QeOWVV2Z93/rWtwbn8zmGKDVTOR52+IEPfKBru+l+1apVXdulgtdee61ruzylWSXdzK7htS5bqAnNj++qq67q2lFlSTejlmZCiyQ2N+mqCbQ006Dj66wJITzROlLKTb8+vppp3WSsz7Rml3SiEEgnqgaoz7ubc3Wdfu+MUQnRKQ3hdhN1a4XI0nDJSLqK8GMtrf4ZrSt6hqJqu47KMjX3eyQPlN6T0b3j75oolDHKBllabdTR80ImRQAAABgdPhAAAACgBx8IAAAA0GOyMEfV7yOfANfvVb/0UMZI07vgggsG94vmVzw0zY9J0XV7eKSioZIp5cf39a9/fbDPj+Gzn/1s1/Zqjnp8rluV+iBERNUcIy0uul5+DOoHEOljPqZu674Euk5PtazbenikaoZ+PvVcn3feeVmf+qm4D4JeT9cPI103qnAY6a66bWmlQl9HaaW7SFOu0dZ1nX5/vP7660VjuP4bacpRWF50XiJKw89qUi1H4+g583MdzRHp4lFYno4ZrSsKSYz8DKIxPYS1xc8mpfieLE1DHvX5OvVe9mdFz0X0Lo18DqK+U02rjgUBAAAAevCBAAAAAD1GkxicKJuhmufdFKtmfg8tjEz5akpx87zKDzWZG6PQKqVUwkgpP76nnnoq69Nj8DG/8pWvdG09npTy43UzklZNdDO7SgdujtRx3PSl4/h8jz32WNd2eUPX4tU/1Uzm++l8Hh6pPProo4P7+ZjPPvts13YJRc+Fm+dVBoquu8tVWrnTz+eOHTu6tptfdX6vdKemTL9++ttNsUOmzChLpK9Lx4+e09KwUN82GtNlO32H+HMaVYHUezfKQOom6SiUseb9otRknFT0GHyMMapQutSjtL5LI6JMio7erxs3bixei8pOLqOVhh3WXK/S8+TPt1KT0bIk3JQwRwAAAGiGDwQAAADowQcCAAAA9Fg2Pz8/37Lj3NxcWrt2bTqQUloz8qIAAABgGuZSSmvTL3wU1qwZ/guOBQEAAAB68IEAAAAAPUYLc1xtIXSzpjQE0sPPNBueh3JpKEsUBuUZ7lrR8BQPv9GQOg/jVKIwuTHWlVJ59cEo81+Uue66664bHFMrXqaU0pNPPtm1PXQyurZaEdOPLwr3Uzx0S0O0PDxL+2oyyZWGSEVr8fk0JExDpLZv355tF50/va88xExDSjds2JD1rVgx/MqJ+jQsy0O0oqyYHsJaiobIto5RUyE1CjnWa/Tt//t/s76PXX/94Jil4ZjROltDGaP9aqoWRqHmekw+X/Tei0IEo1DGKLNoFI6p4cj+d6S0eqSfI11bFM46FK5+5MiRlKQi8RBYEAAAAKAHHwgAAADQgw8EAAAA6DFZquXIJ0DTMJ9xxvA3iqdrjlI0l+L7eUpXRXUe14pUj4o03hr/BNWIrrnmmqxP0716+tpI/1IiX4LSdZ0M1e+jdKR+XnRtd999d9an40S+GdG19bTIke6qlQNr/D1Kcc1+DB+WKBV4abrjzZs3Z7//9E//tGt7qmo91+7Xoz4CPrem23ZtX+9xPyfaF/lbtOLvAfW/qEmnXFpZz313VDO//PLLs74sRbr5ICg1aa2VmvTGpVUgo6qFrqeX+hlEz1vrsdeg6dJrqnEqrc969L6K/LlOFSwIAAAA0IMPBAAAAOgxmsRw6NChpv0iGSHCzZoqaUSVJNV0mFJuWnQzo5qz3UTt8w/hpiGtyuUhL2oaclOUmi6jMKHI1OZr0fCbSEbwanaKhxPp/JGZMTLF+jprwsUUNYO7VKCmzKgqo/fpONHxRZSGNp0MPS/RMWzatCn7feutt3btSy65pGufe+652XZqBn/ttdeyPg0n1MqYKeXhpm7ujCrpRURhvlG4W2QS18qc0fsrGtNpvVej0FqVpP6X9em13LlzZ9a3bdu2ru2m+9LQxprjibaNQi6V6BmK3ntRiLpXpIzCkSN0zJp3m67TpYkoHFOfnUjW8mdsSP7Qf6eaIwAAADTDBwIAAAD04AMBAAAAekwW5qhoKtSUUlq+fPmk80Whk67Vqu/CypUrs74oBFLZsmXLYN+//Mu/ZL9VS4q0oyiUS8NtUso1L98v8klQDThKDRzp8JHu6PvpHK69leptfjy6Fr9eUShspD1G1z3SwktTzUaaaGso17XXXpv9/sM//MOu7amr3//+93dt9SWYm5vLtnv11Ve79qOPPpr1RX4p6mdQo/9G4YoXXHBB165J/6t+No5evyg0LPKXibatCQ/W+V1DfuSRRwb3U7+Dm2++OevTcEkPHdb9/Dkp9feIwq1rrq3S6sPh10jPvd8vum7vi65fdM+Xrs2vbZSqXvG1lO7XGo55HCwIAAAA0IMPBAAAAOgxmsTgZv0o1FAlBzfrR7SGUqqM4Bkeo/lLszVGVQt9jCgMUInM+h4epmbp1qxyvp/O7yGdNWGBSqlpuDVTZGt2TTfr6zG46TmaIzr3NWbqITy75u233961XXZSvMKhhtTpud61a9fgdlEWTje9RiFt0TpP1RyaUnkmQ8ffA9E9F80RHUNUEbZ4O5NsHn74YenK++64446u/fnPfz7r+853vtO177nnnqxPr3UUHuxEmRRLx4iIQgsjKcspDbeuua8jGSEKUVc5zq97VP1WGTNzooMFAQAAAHrwgQAAAAA9+EAAAACAHqP5ILhGorr1wYMHB/dz7S+qEqd6mPdF/gmlfg6RvuxanOpYO3bsKBo/pVijjyoHRqFHqo1Fvgs16H7Rmh0NR4t0wNL0oCcbJ/KV8LTaQ2O6LhjdZ1GKbdUlxwrXUh1Z03Q7GpKYUu53sHfv3qxPKyqqzurpfiN99rnnnhvsUz+DqNJcVNUvCj+r8VWoCTWcekz1VfIwNf0dpVl39J7wa/KFL3yha//lX/5l1vfpT3+6a19//fVZ3xe/+MWurWmzU4rTl8+ioqISVYFsRX0ZojG9L7onx7hfoiqQ0bt06N4h1TIAAAA0wwcCAAAA9BhNYohM2W7qVVOsm2qi8KLSMLnWkLLIFOQmQd3W51MzVU1GMZUtPPROzXdRnxNVuis1CUbXtibzX2m1t0gmac2AGFFjstYwWV+nngvPFKkhTDVhchqC5lUTo6p777zzTtf2cC0NrSoxR6ZUV3lR73/PZKjPjR+rHk+0lqhaXqs51+VFfU5bs9g50X46RyTLOHpf3XjjjVmfhkB+85vfzPpuu+22ru3P8N/+7d92bQ2HTKkfEqlEcl9pCHKUudHfnWPJCkppqKZTmkU1ugdcRojkOMWfhxI5DokBAAAAmuEDAQAAAHrwgQAAAAA9RvNBiELhNm3alP1WfdbTS6o+6uGDpZpQpP9edNFFWZ9qtTWoruua4Uc/+tHBvojWFK4bN27s2nr+fL/o/LkO2RpWVppitSYMsDW1c4SG4l155ZVZ37p167r2+vXrB8c466yzst/nnntu19aKif7btUbV9z3dsYYk7t69O+uLQp/0/oyuiaZvdq37iSee6NpReKejfhqux+qz6M9pK9EzpvP5s6FE2vcUPgcR/h6IUi9H7wz1//AQSA19je7xW265JfutFSM9hbfeq/5e1RDaKIWxUxriHIWhR0TvttIxUsrfZ/5ui66RPnN+H0dri54dHXPoPesVlofAggAAAAA9+EAAAACAHqNJDJEJ0k0sraY3JQppc3OaZhtzM2qU3UxNPtF+3qe/o9CVmgpdUVjl0HYp5WbV1vDPqIpaFGITrc3lo8hcqH1u9tuyZUvX/r3f+73Bda5atSrrU5N/afhsSrnJ082fanL16nIqI5Sa91JK6ZxzzunablZUE6TfL9F1UElK78/t27dn22l4sj9vmh11+fLlWZ9ePzdxDoVYOmNUv0ypPFTM0WsbyXZRyGXNMagJPhrT+fd///fB/aLQVJUD9B7zPpcKIvlNpZAVK/I/LTfccMNgn2bBvf/++7O+bdu2nfgAUhz+HFVe1GvrfVHo8NB2/rtUUkip/54YWqe/9/Rvld+fOubQu40wRwAAAGiGDwQAAADowQcCAAAA9Fg2Pz8/37Lj3NxcWrt2bTqQUlqTUjrTKiZqCNjll1+e9bWG0JVW74p0pUhPV202pXIfgRqNUnVB15V0fq+sp7gWHfk1RFpVpFFGIT7R9dM+D89SvTvyE/nUpz6V9UXapvoW1KS1Vg3dK4FqSmPXYCPdX4lCEFt9cKL7MdI9XWPWY4qOR+fze0dD1XxuDa+LzpET+fwMbefbtqZarpmvdJwpKkn+n69/Pfv9F5/7XNF+0XPq98fUuC6u83t4sD6LTz/9dNan9+CePXsG56tJ31xK9DcmIvIvqQlD178BpZVxdfxjx46lvXv3pgMHDqQ1a9YMzosFAQAAAHrwgQAAAAA9RgtzPOOM/FvDKzgqNZXhlNLMWZEZx01Duq2bhaNQxlJpxLeL1lkakuiZyCLpRY/XK+uVZgpz01cUQqTrdrO0SgVaMS6llJm51PyfUi4BeIjgf/3Xf3VtD0t65ZVXunYUptpKFBbbmiWwxtwbhWRFJk/t07A1H0P7nCgDaZSZb2o0Q2BK+TH58ag52/eLzN5j4FkIozmybU1i0GOK7h0NXXT8vETXtlWO0OPzY4+44oorBvuiCqmRzKv7uZRbU8FxiEjG9r8HpRllazLP6vFqyKOu4+jRo717/kRgQQAAAIAefCAAAABADz4QAAAAoMdoPggbNmzIfrverWhIivsSKF6pT3XkmupyrUThaKUhkJGvQk2YYeSToH4Hfs7UR2CsqoxDc6eUH5P7J9xxxx1d28MV77zzzsE5ovC30j6/XtH184qfpUS6f42GODSma/ua4th9Mzz98RC6nz/Dfo2GKN3uVNDUsJ42e25urmtv3bo169P78aabbsr6Pv3pT3dtP7dThCgqXglU8eOL+PKXv9y1//zP/zzr0+vpPg7R/RFVj1Si6piRT4WPr/ePpwC+6667Bsf5yEc+0rU9lD5KJa0+FlE4uROFR+r7siYkUffzd7f+bayZT9H9PMyxBCwIAAAA0IMPBAAAAOgxmm1QM+GdjE2bNjXNEckKap657LLLBrerCR8s3c/R0BI3DbkZSdHji8xNEb5dlEUyWkvL3I7LTJoVTUMQU0rp0ksv7dpXXXVV1qdmcA+RUlNiZOKPwrPGCmPTsKGHHnoo69PqlVElUkfDha+55pqs77bbbuvabrJWU21p9ciaKpNDc52MyJxcKlV4GKyyefPm7Lfegy4deQbNIWrWGZ2LMY7PUdnEZZLoepZeB1+zblsqRdTga9aMqzV/Y/R597DNqEJqFMKtRDKCo+dJZRHviyQbr/oYVY9UKTlKDVACFgQAAADowQcCAAAA9OADAQAAAHqM5oNQU6FOqxZG4UReXVHT47q/gGpAvpYoZXLkL6Ba8Zlnnpn1XXDBBYP7qe7j82l4iaen1r5IH63RkkrDFd2/w4+3lC1btnRt9StIKaXdu3d37SeffDLrU30x0gzdd0F1u5qKfNH9qvdLlE7Z53vkkUe6dkka0xJUj/YxH3jgga79x3/8x1mfn3tlKJVulFp5LEpT9Ub3QJQK2FPz6m/3X1GfGKf0XLSuM6Imbfb111/ftf149Bjcz+aJJ57o2lHFUg8fVD8H93vRd42ndo7CddXvwI9BfUj8WdR1R8fg2r6O434U6ufjffq8u89BlLJc1+bX8rrrrktD6DhR2gBnaJ163EeOHCHVMgAAALTBBwIAAAD0GE1icFNJlDmutVqY7ufmHw318IyBahZ2M1UUZhJVBItMzbrfGNXBUspDN/3YSysTRtJLFOLp6LV1k5meQzW5p5SHDfl+Ggao7ZTKwzFbcckmmk+3rZERNHOdS0vKypUrs99Hjhw5YTulXH741re+lfV95jOf6dqR3KD3amulvlaid4TfH1E1wtIxHX2fRNn2IpO/97VmzIyIxty1a1fXvuGGG7I+Nd1/7Wtfy/q2b9/etSM50+9VDe/77Gc/m/WpnKPrSimXBnfu3Dk4n79L1bQehZr7O79UenSiCpHR/KX3p2+n647W6ZJGJLvq3yZ99vWeLg3xxYIAAAAAPfhAAAAAgB58IAAAAECP0XwQXMOLwn2iEKJIF9RQHQ9Z0v0+9KEPFY1/KrT6WEREPg/qIxBVJowqGjqqa0UhPT5GpJVpdbTI/6K0mpjjoVWl4ZjuZxClUS2t2uZEfgZ6Dj3VeFSZTSufum+Easyeolb9P0orj7qWGfn11KQeV0qfjbGe0wg9Jj++aP5ZrK2Uj3/84137qaeeyvq+9KUvde0ofXNU2dH9XtTv5q//+q+zvt/5nd/p2q6Zl/o4+X2lflJRSmH3Hyt9f/m7TMeJQqNr3olRmL0egz7rKeWh9BGRD9rQeSithowFAQAAAHrwgQAAAAA9RpMYoop4NdXySrf17Vr3a8WzgSkqf0TmSJcpojCz0sx4NabfaD41TUVjusnsyiuv7NpuMlMTeVRpzk2euq2bSjVcx+WHobn9dxTm6KY4Nbn6OnV+lz5uvvnmrn3LLbdkfVFGPz33mv3O1+nnU026XgluSJJyU2VUXa4mdGypE8l2U/TVcP/993ftv//7v8/69J5YtWpV1qdVQp1Sic3lhx/96Edd29+P0Zja5+9E3VYrO6YUZ5gs5fnnnx9cS5S9sDS0PKX2Z0Xn9/lKx9T3gMo+fu2GwIIAAAAAPfhAAAAAgB58IAAAAECP0XwQlgoeHhmFTkaU+lxE29Wkto3GafV5iDQ81auidbp/gm77iU98IuvbunVr1458EBzV+qP9SnU13zYK+akZU3E/A6+Kp3gFwlI0Xa77Zuh5ch8E1TajUL8a35NSptDhIzQ0rUY3VsbSm1v7Iu6+++7BPvU7cJ8YT5Os6HvhoYceyvr0dxR26+HAHr6rvP7661078gfy+XQ/DwnUY/D7P3rvjeHX4Cnto/BIXZsfe1QGQH0nNA2/z6Fj6DudVMsAAADQDB8IAAAA0ONXTmJoDbmskR+ibUvlh2jMKMQzGjMKzYyq2TmlmTBdYlAz6ve///2sL8oKGBHtp9JEZFLzPg1XrMn4GGV11PMbVYH066Amz1YTvJpifcyoMqdLDkprlsXSDHdjyQ0qD7jpN8qap/h+syZam967nr1TQxm1umdKKd14441d2yW2o0ePDs6n5+Kee+7J+vT5i6S5qFqq90Xynz5vHjqpIdZuuo+yobZmCFVcklIzf5RJ0WUYzUobZe51CeWjH/1o1/aMlrVgQQAAAIAefCAAAABADz4QAAAAoMeC+CBEevpCMkW6aPdHiPwTpqgSF62lNASz9Rr5fnfccUfX9jSmqqO5pq16omuSkW9BqS+Dp0zWMaMxanwl9Nq6nqhVFC+66KKsT/0CvOKmarBRtT7XdXfu3HnC7byao45fWv3NKa22OQuiEDrvG4OoYqjTGl6n96D7IKim/clPfnJwbXNzc4NjbtiwIevbt2/fCcdPKfateeONN7p25J8Q+fx4KnW9Zjq+4/e/7hddo8gHx4nCDtVvw8MV1efBfU127NgxOF8UNqo+EENVNEm1DAAAAM3wgQAAAAA9fuXCHCNKwxNPBTU1R2GHEb6WaN2lfS5vqMTgY0RSSLSfotUNT4aaLqNwHzeRa3W7saSCCDVderiUmmY9lFFlBT+30bluzb6n5kU16bqJujWLpI7pEpDP0TJm6xhjZTJcuXJl0XaRvOKSjd+7Y6D3oIcuqqzgZu/oHaUhzjUSQ2nmPkdlBa9AGZnZFZcYVI6IMpyOFQ7ZmsFTj8/nU7kxqvQ4dF9HoawKFgQAAADowQcCAAAA9OADAQAAAHosiA/CYgptLGWK8MQabX+M+Zwoza5q5q1jRvj4pWFeNZXY/uiP/qhr33fffVnfWH4HioYUPfPMM1mfhnXWpLXWUKtt27ZlfarLa+W+k1EavtgaotgaMqhacWtYpTNFmKWuzf001D9hrGMo1e+jEEG/31WDjlK3O+qTEGn0HjqsKX8j34Go4myk+3tKYQ0tfOSRRwb3a02t7CGQ11xzTdF+UXpxZ+PGjfULS3nIuKag9vDLErAgAAAAQA8+EAAAAKDHTCSGmvDBhcyyuBSlj5RiqaDU5B9tV2oCd9x8VhruE5ndovnc5KmmTDdBRiFZragZd/v27VlfVJFPw8w8W6LKCp4VTUPANm/ePDh+TbXFIVqrHUb7Rde5NTQsoqYqY2kVSK9KGmW0bCULNbTKfWrKdxlBs/t985vfzPpuu+22E47hvPTSS9nvxx9/vGv7/ajrdJN76z2itIb1btmyJftdem/5MUTVRvX4orBDR59NH1N/eyiqjlma8VHHI5MiAAAANMMHAgAAAPTgAwEAAAB6jOaDUBMG2JpiOPJPON18F6Ix/Vx72NxQ3xTVIjX1akq5L0PNfKqjuQ4faX8RkV+FhgWOURHScS363nvv7dqu+aovga8lml+3dS08SkdcmipYmXXIo6NV9zR0y/tqiEJm9d7xyqPKxRdfnP3+8Y9/3LVb7x1fV6aFmw+C+ta4X43+9nP07LPPdm1P86zphx966KGs74EHHujafm1vuummE46RUkpbt27t2uobkVLuKxSF9kW+AzX+JYr7B+j7ZazU3NE7y991Q3PU+HCUhEeuWFH2px8LAgAAAPTgAwEAAAB6LJufn59v2XFubi6tXbs2HUgprUkpfePOO7N+NS9HJvDSyoA1Y9YQVS6LZBNlConB516qIZhjUHodxpJQoiySHtqleCiSUhqKVJNJrlRucfNkSwhha+hpaZjtyXBZYQiv3KdmcO8rpTXbpO83VEUzpbhqoa77jTffzPpWy3uwJsRSZS6XCnTdLnnpOv/gD/4g64vM4HrP1ZjnXzZJRSl9Nny+oUyDJyOSslSGisKKayTSiCiscmg7nfvw4cPprrvuSgcOHEhr1qwZ3B8LAgAAAPTgAwEAAAB68IEAAAAAPWaSajnShls1Sh8zCufTvkjLnyL9bkQUqumUpquuSWsdjT8Lv4rS+UrX4ttFaWiHtksppQ0bNhTNVxPGFqWznaKyZDT31PNNTWsV1BrG8p1oITyeL30p+6l+ADUhe5EOrz4P7g8RVS0s9W2J0hRH29b40kR+DlF4q6Zrj3xWvCKl+h24P4T2uX+CzhdVuXR0Dh+zJESXVMsAAADQDB8IAAAA0GM0iaHVJF0TAhnJCJFZTvtqwipbKZ2j1VTq6yyVRqJrVGOmHUN+mEXYZqkpfRYm99I5auSAUgnldJMw/N7R32OtRWWmKY4nWmckR7nEcPvtt59wjJr5W49vLDkzQseZhQTcmgk2QiUAlz5Ksh6m1JdlouyyY4ZVYkEAAACAHnwgAAAAQA8+EAAAAKDHaD4IkT7UmjLZ+1SPmiK0aYoxozlafR5a19mq4UX+HtG1baVVv1zodNSRrjuFv0CkW4/hu9Cyjhqm8F0Yy19gaj+Kse6BKL139DyMcXytz1trJd7o78FYaLr9KPQ76vP3rFe8HaImtFbfsx7mGFWIPE6UPlzBggAAAAA9+EAAAACAHqNJDFEVutIQxJNRmpExypQVrXMWYyqt5vgpzPo185XO3brOWYQzlVbxrDFjlpqNa7JWlkoqrdJEKa3m1rHXcTJa1zIL6ar1vooYI5zQx9Dnr0YGLb0novDuhc4uq9vWhNIrrVWBa96X+jfGJYwSqaK0iiUWBAAAAOjBBwIAAAD04AMBAAAAeozmg1Cqu49Fq+5fs84pxoyIUkkr3heF5szaP6E0rbUzhj9GzRilKa+jdN8RNfvVpJ4dY4wW/btmn9bjGUNPb73nWq9PjWbest2J5hgaJ0pBXbMW19CjbYeoCWWcwl8govUdVeqLVfN+bp2vZQxdFz4IAAAA0AwfCAAAANBjNInBiUy/paaTKPwmMuvXmGbGCJ2sITLfKa0maj9nrdUrW0OWlLHCfZSa0KMx5ICxqlxGfXpf15h3xw6bG8uUXmoSr5HRWkNtW7aroSYkt/SdWHMtS49pCvN863atz2yNeb50jikkKScKG506K+2pjoEFAQAAAHrwgQAAAAA9+EAAAACAHpOlWi6tTFUTrqj67FjpeGuqSZZSqvtE4XVR5bJIi2utqulMUSmtVKOPjq/mGFqrhkZ9peel5vyV+qVMkfK3dfwxKvm1+mz4+kt9aWr6InT+KSrVRvM5Y4Qk1rxrWt+JrT4kredljBDBKcZs9cerQf+mDp3bQ4cOFY2FBQEAAAB68IEAAAAAPUaTGF555ZXBvrVr12a/W83C+/btK1pLJG+MFa7YWumxtM8plS1qqmq2rm2M6+C0XpfW7JOl+/k92BpaGIU6TRGiGFWrLB2jNDRziuNppbUa4BjhsrXbKq2SaWnlRR9/6syeY52z0uNrDW1vvV41WStbQ4JLZVen5N1GJkUAAABohg8EAAAA6MEHAgAAAPQYzQfB/QwiDhw4UDTOWKlRo/nG0B5dk4/2i0JQdL8a/T6idZypq0C2ztfql+L7tV6HMe4XH7P0GNxPo9QPJtKfIz22VBdvTX89RhrwlHJ9tvRYx2IK34UaX6FWIn279Zhar3urH9FCUhOiXjPOEDXVMaP9jnPw4MGiebEgAAAAQA8+EAAAAKDHaBJDZFZx8+e6deu6dk0mxVai+WrWreg4rSE2kfQxi6pfY1TcdPScTSFvtB6r76fnPgrDja6RUyqdtTLWeRm6tlOYsseqyqj7uSQTPael8k3EFOelhtYw6ojWsMqxnr8xxmyVjxZSdppizBZJ6L333iuaFwsCAAAA9OADAQAAAHrwgQAAAAA9RvNBaPUdUP8Ap0ZbibYtTTU7FpEOr+uuOYbWNKOtaa1b16Ka7xSVyqK11Gh/kU/A1L4EC02LRtp6j40x96ns1zrGGDr1GH4hNUT+CK3v0tZqldH8Y71rSo+3dYwp/OFqKA3FdqL0+sch1TIAAAA0wwcCAAAA9BhNYmg11YxlhtO+mopurXOXmiCj7HfRHIs1k6Fv23peWqkJO5xinNL9XIqIwipLx6/Zr2WOaJ+xTNRTsFRC1ZSx7uNS0300X+tappBexjovilcaLq08XFOhWKl5FkvxMaPzFK37OIcPHy6aFwsCAAAA9OADAQAAAHrwgQAAAAA9RvNBiHSPsfwMoup1pdXfospX7qsQVTyL5i493sg/IQr/jJhFWtjWyostYzhjhRmWjtOqJ0b7Te3/ULPtGMczdQroGsa650p9RmoovSZT+AS0+r3UMEZ48BRVgadgimfReffdd5v2K+HIkSNF22FBAAAAgB58IAAAAECP0SSGVlrDb1qzb0XShI/ZOofiWRyjrI5jZHz0MVQaqTGxloZjtuJjlFaBHKvqZOm1bZWPIhNnjfmz1WTeYiKfhdm0NZQyovQeqLmWU5ioS838UYhstF/rWlpD9qaQJmqOfQrZZIxwZKf0nE1xz6lMcfbZZ3dtwhwBAACgGT4QAAAAoAcfCAAAANBj2fz8/HzLjnNzc2nt2rXpQEppzciLAgAAgGmYSymtTb/wiVizZvgvOBYEAAAA6MEHAgAAAPQYLczxf//Zn401VIdnktIwjYjWcJGakLmoQmU0ZmvoZJQBcinimSn1vNRkgyytZjcFU8w362MYg1lUrxtj/CmyYp5s2yFaj+GLX/pS9vsvPve5wW2jEO4p1jlGJdcpqjnOmihUs/Vv2tgcPnw4pbvuOul2WBAAAACgBx8IAAAA0GM0iSEyq4yVHatm/iFaTbg1JrNS+aFmjsiUXjrGGJkax8IlE/0drbNGmpjC5FlapMhpzfg4RcGrIWqey6lNwbPOZOiMkU2zZv7W4lBTZLTUMWvuidLz0ir1zPoebCVa10JJCq1gQQAAAIAefCAAAABAj2aJ4Xh+pbn///vQoUNZvxaDiPoc3ba0oMSJ5hhixYr8kHU/74vGj7b92c9+1rWXL18+2NeKj1k6/sGDB0957oXmvffey37r8UbHXnp/OH4Pto4T3S+ttK5liNLnsnbbpUjNu0BpPS/R+1KZs99TXIfS9+4Uz9RY9yDEHD93J8uT2JxJcd++fYtK0wYAAIBy9u7dm9atWzfY3/yBcOzYsbR///60evXqtGzZsuYFAgAAwOyYn59Pb7/9drrwwgvTGWcMexo0fyAAAADA6QtOigAAANCDDwQAAADowQcCAAAA9OADAQAAAHrwgQAAAAA9+EAAAACAHnwgAAAAQA8+EAAAAKAHHwgAAADQgw8EAAAA6MEHAgAAAPTgAwEAAAB6/D92yhjnEbgM9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for img_name, data in list(images_detection.items())[5:]:\n",
    "    image = torch.from_numpy((255 * data[0]).astype(np.uint8))\n",
    "    print(image.shape)\n",
    "    bboxes = data[1]\n",
    "    bboxes_coords = boxes_to_coords(bboxes)\n",
    "    \n",
    "    predicted_bboxes = detections[img_name]\n",
    "    predicted_coords = boxes_to_coords(predicted_bboxes[:,:4])\n",
    "    \n",
    "    show(draw_bounding_boxes(image.view(1, *image.shape), torch.Tensor(predicted_coords), colors='red'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = get_detections(detection_model, images_detection_no_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(137, 275)\n",
      "(2, 2, 4)\n"
     ]
    }
   ],
   "source": [
    "def get_detections2(detection_model, dictionary_of_images):\n",
    "    \"\"\"\n",
    "    :param detection_model: trained fully convolutional detector model\n",
    "    :param dictionary_of_images: dictionary of images in format\n",
    "        {filename: ndarray}\n",
    "    :return: detections in format {filename: detections}. detections is a N x 5\n",
    "        array, where N is number of detections. Each detection is described\n",
    "        using 5 numbers: [row, col, n_rows, n_cols, confidence].\n",
    "    \"\"\"\n",
    "    detections_dict = {}\n",
    "    threshold = 3\n",
    "    n_rows, n_cols = 7, 12\n",
    "\n",
    "    width = 370\n",
    "    height = 220\n",
    "    \n",
    "    box_height = height // n_rows\n",
    "    box_width = width // n_cols\n",
    "    \n",
    "    \n",
    "    for image_filepath, orig_image in list(dictionary_of_images.items())[1:]:\n",
    "        #image = np.zeros((height, width))\n",
    "        #image[:orig_image.shape[0], :orig_image.shape[1]] = orig_image\n",
    "        print(orig_image.shape)\n",
    "        orig_image = np.random.randn(40, 100)\n",
    "        torch_image = torch.from_numpy(orig_image)\n",
    "        torch_image = torch_image.view(1, 1, *orig_image.shape).double()\n",
    "        feature_map = detection_model(torch_image).squeeze(dim=0).detach().numpy()\n",
    "        \n",
    "        print(feature_map.shape)\n",
    "        break\n",
    "\n",
    "model = model.double()\n",
    "get_detections2(detection_model, images_detection_no_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "import torchvision.transforms.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxes_to_coords(bboxes):\n",
    "    bboxes_coords = []\n",
    "\n",
    "    for box in bboxes:\n",
    "        row, col, nrows, ncols = box\n",
    "        \n",
    "        xmin = col\n",
    "        xmax = xmin + ncols\n",
    "        \n",
    "        ymin = row\n",
    "        ymax = ymin + nrows\n",
    "        \n",
    "        bboxes_coords.append([xmin, ymin, xmax, ymax])\n",
    "    \n",
    "    return bboxes_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([137, 275])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'test-1.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [479]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m bboxes \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      7\u001b[0m bboxes_coords \u001b[38;5;241m=\u001b[39m boxes_to_coords(bboxes)\n\u001b[1;32m----> 9\u001b[0m predicted_bboxes \u001b[38;5;241m=\u001b[39m \u001b[43mdetections\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimg_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     10\u001b[0m predicted_coords \u001b[38;5;241m=\u001b[39m boxes_to_coords(predicted_bboxes[:,:\u001b[38;5;241m4\u001b[39m])\n\u001b[0;32m     12\u001b[0m show(draw_bounding_boxes(image\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39mimage\u001b[38;5;241m.\u001b[39mshape), torch\u001b[38;5;241m.\u001b[39mTensor(predicted_coords), colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'test-1.png'"
     ]
    }
   ],
   "source": [
    "#draw_bounding_boxes(image, boxes, labels, colors=colors, width=5)\n",
    "\n",
    "for img_name, data in list(images_detection.items())[1:]:\n",
    "    image = torch.from_numpy((255 * data[0]).astype(np.uint8))\n",
    "    print(image.shape)\n",
    "    bboxes = data[1]\n",
    "    bboxes_coords = boxes_to_coords(bboxes)\n",
    "    \n",
    "    predicted_bboxes = detections[img_name]\n",
    "    predicted_coords = boxes_to_coords(predicted_bboxes[:,:4])\n",
    "    \n",
    "    show(draw_bounding_boxes(image.view(1, *image.shape), torch.Tensor(predicted_coords), colors='red'))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
